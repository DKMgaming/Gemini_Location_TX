import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import radians, sin, cos, sqrt, atan2
import matplotlib.pyplot as plt
import folium
from streamlit_folium import st_folium
import io

# --- H√†m t·∫°o d·ªØ li·ªáu t·ªïng h·ª£p ---
def generate_synthetic_data(num_samples, num_receivers, noise_level=0.1):
    st.write(f"ƒêang t·∫°o {num_samples} m·∫´u d·ªØ li·ªáu v·ªõi {num_receivers} tr·∫°m thu...")

    # T·ªça ƒë·ªô ngu·ªìn ph√°t x·∫° ng·∫´u nhi√™n (target)
    source_coords = np.random.rand(num_samples, 2) * 100  # Ngu·ªìn trong ph·∫°m vi 100x100

    # T·ªça ƒë·ªô tr·∫°m thu ng·∫´u nhi√™n
    receiver_coords = np.random.rand(num_samples, num_receivers * 2) * 100

    # RSSI v√† AoA (m√¥ ph·ªèng ƒë∆°n gi·∫£n)
    rssi_data = np.zeros((num_samples, num_receivers))
    aoa_data = np.zeros((num_samples, num_receivers))

    for i in range(num_samples):
        for j in range(num_receivers):
            rx_x, rx_y = receiver_coords[i, j * 2], receiver_coords[i, j * 2 + 1]
            src_x, src_y = source_coords[i, 0], source_coords[i, 1]

            distance = np.sqrt((src_x - rx_x)**2 + (src_y - rx_y)**2)
            rssi_data[i, j] = 100 - 20 * np.log10(distance + 1e-6) + np.random.normal(0, noise_level * 10)
            angle = np.degrees(np.arctan2(src_y - rx_y, src_x - rx_x))
            aoa_data[i, j] = angle + np.random.normal(0, noise_level * 5)

    # G·ªôp d·ªØ li·ªáu ƒë·∫ßu v√†o
    X = np.hstack([receiver_coords, rssi_data, aoa_data])
    y = source_coords

    # T·∫°o t√™n c·ªôt cho dataframe
    feature_names = []
    for j in range(num_receivers):
        feature_names.append(f'rx_{j+1}_x')
        feature_names.append(f'rx_{j+1}_y')
    for j in range(num_receivers):
        feature_names.append(f'rssi_{j+1}')
    for j in range(num_receivers):
        feature_names.append(f'aoa_{j+1}')

    df_X = pd.DataFrame(X, columns=feature_names)
    df_y = pd.DataFrame(y, columns=['source_x', 'source_y'])

    st.success("T·∫°o d·ªØ li·ªáu t·ªïng h·ª£p th√†nh c√¥ng!")
    return df_X, df_y

# --- H√†m t√≠nh kho·∫£ng c√°ch Haversine (cho t·ªça ƒë·ªô ƒë·ªãa l√Ω, gi·∫£ ƒë·ªãnh cho demo) ---
def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371000
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    return R * c

# --- Giao di·ªán Streamlit ---
st.set_page_config(layout="wide", page_title="·ª®ng d·ª•ng Hu·∫•n luy·ªán M√¥ h√¨nh ƒê·ªãnh v·ªã RF")

st.title("·ª®ng d·ª•ng Hu·∫•n luy·ªán M√¥ h√¨nh ƒê·ªãnh v·ªã Ngu·ªìn Ph√°t x·∫° RF")
st.markdown("""
·ª®ng d·ª•ng n√†y cho ph√©p b·∫°n t·∫°o d·ªØ li·ªáu t·ªïng h·ª£p, hu·∫•n luy·ªán m√¥ h√¨nh d·ª± ƒëo√°n t·ªça ƒë·ªô ngu·ªìn ph√°t RF, d·ª± ƒëo√°n v√† xu·∫•t k·∫øt qu·∫£.
""")

# --- Sidebar ---
st.sidebar.header("C·∫•u h√¨nh D·ªØ li·ªáu & M√¥ h√¨nh")

# T·∫°o d·ªØ li·ªáu
st.sidebar.subheader("1. T·∫°o D·ªØ li·ªáu T·ªïng h·ª£p")
num_samples = st.sidebar.slider("S·ªë l∆∞·ª£ng m·∫´u d·ªØ li·ªáu", 100, 5000, 1000)
num_receivers = st.sidebar.slider("S·ªë l∆∞·ª£ng tr·∫°m thu", 2, 10, 4)
noise_level = st.sidebar.slider("M·ª©c ƒë·ªô nhi·ªÖu", 0.0, 1.0, 0.1)

if st.sidebar.button("T·∫°o D·ªØ li·ªáu"):
    X, y = generate_synthetic_data(num_samples, num_receivers, noise_level)
    st.session_state['X'] = X
    st.session_state['y'] = y
    st.session_state['data_generated'] = True
    st.sidebar.success("D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫°o!")
else:
    if 'data_generated' not in st.session_state:
        st.session_state['data_generated'] = False

if st.session_state.get('data_generated', False):
    st.subheader("üìä D·ªØ li·ªáu T·ªïng h·ª£p (5 d√≤ng ƒë·∫ßu ti√™n)")
    st.dataframe(st.session_state['X'].head())
    st.dataframe(st.session_state['y'].head())

    # Ti·ªÅn x·ª≠ l√Ω
    st.sidebar.subheader("2. Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu")
    use_scaler = st.sidebar.checkbox("Chu·∫©n h√≥a (StandardScaler)", True)
    use_pca = st.sidebar.checkbox("Gi·∫£m chi·ªÅu (PCA)", False)

    X_processed = st.session_state['X'].copy()
    y_processed = st.session_state['y'].copy()

    if use_scaler:
        scaler = StandardScaler()
        X_processed = pd.DataFrame(scaler.fit_transform(X_processed), columns=X_processed.columns)

    if use_pca:
        max_pca_components = min(X_processed.shape[1], num_samples - 1)
        if max_pca_components > 0:
            pca_components = st.sidebar.slider("S·ªë th√†nh ph·∫ßn PCA", 1, max_pca_components, min(5, max_pca_components))
            pca = PCA(n_components=pca_components)
            X_processed = pd.DataFrame(pca.fit_transform(X_processed))
        else:
            st.sidebar.warning("Kh√¥ng th·ªÉ √°p d·ª•ng PCA.")

    test_size = st.sidebar.slider("T·ª∑ l·ªá ki·ªÉm tra", 0.1, 0.5, 0.2, 0.05)
    X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=test_size, random_state=42)

    # M√¥ h√¨nh
    st.sidebar.subheader("3. M√¥ h√¨nh & Si√™u tham s·ªë")
    model_choice = st.sidebar.selectbox("Ch·ªçn m√¥ h√¨nh", [
        "Random Forest Regressor",
        "MLP Regressor (Neural Network)",
        "Support Vector Regressor"
    ])

    model = None
    if model_choice == "Random Forest Regressor":
        n_estimators = st.sidebar.slider("S·ªë c√¢y", 50, 500, 100, 50)
        max_depth = st.sidebar.slider("ƒê·ªô s√¢u t·ªëi ƒëa", 5, 50, 10, 5)
        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)

    elif model_choice == "MLP Regressor (Neural Network)":
        hidden_layer_sizes = st.sidebar.text_input("K√≠ch th∆∞·ªõc l·ªõp ·∫©n", "100,50")
        hidden_layer_sizes = tuple(map(int, hidden_layer_sizes.split(',')))
        max_iter = st.sidebar.slider("S·ªë v√≤ng l·∫∑p", 100, 1000, 200, 50)
        learning_rate_init = st.sidebar.slider("T·ªëc ƒë·ªô h·ªçc", 0.0001, 0.1, 0.001, 0.0001, format="%f")
        model = MLPRegressor(
            hidden_layer_sizes=hidden_layer_sizes,
            max_iter=max_iter,
            learning_rate_init=learning_rate_init,
            early_stopping=True,
            random_state=42
        )

    elif model_choice == "Support Vector Regressor":
        C = st.sidebar.slider("C", 0.1, 10.0, 1.0, 0.1)
        epsilon = st.sidebar.slider("Epsilon", 0.01, 1.0, 0.1, 0.01)
        kernel = st.sidebar.selectbox("Kernel", ["rbf", "linear", "poly"])
        model = SVR(C=C, epsilon=epsilon, kernel=kernel)

    # Hu·∫•n luy·ªán m√¥ h√¨nh
    st.sidebar.subheader("4. Hu·∫•n luy·ªán M√¥ h√¨nh")
    if st.sidebar.button("B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán"):
        with st.spinner("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh..."):
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)

        st.subheader("K·∫øt qu·∫£ Hu·∫•n luy·ªán")
        st.write(f"**Mean Squared Error (MSE):** {mse:.4f}")
        st.write(f"**Mean Absolute Error (MAE):** {mae:.4f}")

        # V·∫Ω bi·ªÉu ƒë·ªì scatter gi·ªØa y_test v√† y_pred
        fig, ax = plt.subplots()
        ax.scatter(y_test['source_x'], y_test['source_y'], label="Th·ª±c t·∫ø", c='blue')
        ax.scatter(y_pred[:, 0], y_pred[:, 1], label="D·ª± ƒëo√°n", c='red', alpha=0.6)
        ax.set_xlabel("X")
        ax.set_ylabel("Y")
        ax.set_title("V·ªã tr√≠ ngu·ªìn ph√°t x·∫°: Th·ª±c t·∫ø vs D·ª± ƒëo√°n")
        ax.legend()
        st.pyplot(fig)

        # L∆∞u m√¥ h√¨nh v√† d·ªØ li·ªáu d·ª± ƒëo√°n ƒë·ªÉ d√πng sau (n·∫øu mu·ªën)
        st.session_state['model'] = model
        st.session_state['X_test'] = X_test
        st.session_state['y_test'] = y_test
        st.session_state['y_pred'] = y_pred

    # Ph·∫ßn d·ª± ƒëo√°n v·ªõi d·ªØ li·ªáu test
    if st.session_state.get('model', None):
        st.subheader("D·ª± ƒëo√°n v·ªõi t·∫≠p Test")
        if st.button("Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n v√† b·∫£n ƒë·ªì"):

            output_df = st.session_state['X_test'].copy()
            output_df = output_df.reset_index(drop=True)
            output_df['source_x_thucte'] = st.session_state['y_test'].reset_index(drop=True)['source_x']
            output_df['source_y_thucte'] = st.session_state['y_test'].reset_index(drop=True)['source_y']
            output_df['source_x_du_doan'] = st.session_state['y_pred'][:, 0]
            output_df['source_y_du_doan'] = st.session_state['y_pred'][:, 1]

            st.write("K·∫øt qu·∫£ D·ª± ƒëo√°n (5 m·∫´u ƒë·∫ßu ti√™n):")
            st.dataframe(output_df.head())

            # N√∫t xu·∫•t file CSV
            csv_buffer = io.StringIO()
            output_df.to_csv(csv_buffer, index=False)
            st.download_button(
                label="‚¨áÔ∏è T·∫£i file k·∫øt qu·∫£ CSV",
                data=csv_buffer.getvalue(),
                file_name="ket_qua_du_doan.csv",
                mime="text/csv"
            )

            # V·∫Ω b·∫£n ƒë·ªì Folium
            st.markdown("### üó∫Ô∏è B·∫£n ƒë·ªì V·ªã tr√≠ Th·ª±c t·∫ø v√† D·ª± ƒëo√°n")

            # Gi·∫£ ƒë·ªãnh t·ªça ƒë·ªô trung t√¢m cho hi·ªÉn th·ªã b·∫£n ƒë·ªì
            lat_base, lon_base = 20.0, 105.0
            scale = 0.01

            m = folium.Map(location=[lat_base, lon_base], zoom_start=13)

            for i in range(len(output_df)):
                lat_true = lat_base + output_df.loc[i, 'source_y_thucte'] * scale
                lon_true = lon_base + output_df.loc[i, 'source_x_thucte'] * scale

                lat_pred = lat_base + output_df.loc[i, 'source_y_du_doan'] * scale
                lon_pred = lon_base + output_df.loc[i, 'source_x_du_doan'] * scale

                # Marker th·ª±c t·∫ø
                folium.CircleMarker(
                    location=[lat_true, lon_true],
                    radius=5,
                    color='blue',
                    fill=True,
                    fill_color='blue',
                    fill_opacity=0.6,
                    popup=f"Th·ª±c t·∫ø #{i+1}"
                ).add_to(m)

                # Marker d·ª± ƒëo√°n
                folium.CircleMarker(
                    location=[lat_pred, lon_pred],
                    radius=5,
                    color='red',
                    fill=True,
                    fill_color='red',
                    fill_opacity=0.6,
                    popup=f"D·ª± ƒëo√°n #{i+1}"
                ).add_to(m)

                # ƒê∆∞·ªùng n·ªëi th·ª±c t·∫ø v√† d·ª± ƒëo√°n
                folium.PolyLine(
                    locations=[[lat_true, lon_true], [lat_pred, lon_pred]],
                    color='gray',
                    weight=1.5,
                    opacity=0.5
                ).add_to(m)

            # Gi·ªØ b·∫£n ƒë·ªì hi·ªÉn th·ªã ·ªïn ƒë·ªãnh v·ªõi key
            st_data = st_folium(m, width=700, height=500, returned_objects=[], key="ban_do_dudoan")

else:
    st.info("Vui l√≤ng t·∫°o d·ªØ li·ªáu tr∆∞·ªõc r·ªìi hu·∫•n luy·ªán m√¥ h√¨nh ƒë·ªÉ d·ª± ƒëo√°n.")

