import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import radians, sin, cos, sqrt, atan2
import matplotlib.pyplot as plt

# --- H√†m t·∫°o d·ªØ li·ªáu t·ªïng h·ª£p ---
def generate_synthetic_data(num_samples, num_receivers, noise_level=0.1):
    st.write(f"ƒêang t·∫°o {num_samples} m·∫´u d·ªØ li·ªáu v·ªõi {num_receivers} tr·∫°m thu...")

    # T·ªça ƒë·ªô ngu·ªìn ph√°t x·∫° ng·∫´u nhi√™n (target)
    source_coords = np.random.rand(num_samples, 2) * 100  # Ngu·ªìn trong ph·∫°m vi 100x100

    # T·ªça ƒë·ªô tr·∫°m thu ng·∫´u nhi√™n
    receiver_coords = np.random.rand(num_samples, num_receivers * 2) * 100

    # RSSI v√† AoA (m√¥ ph·ªèng ƒë∆°n gi·∫£n)
    rssi_data = np.zeros((num_samples, num_receivers))
    aoa_data = np.zeros((num_samples, num_receivers))

    for i in range(num_samples):
        for j in range(num_receivers):
            rx_x, rx_y = receiver_coords[i, j * 2], receiver_coords[i, j * 2 + 1]
            src_x, src_y = source_coords[i, 0], source_coords[i, 1]

            distance = np.sqrt((src_x - rx_x)**2 + (src_y - rx_y)**2)
            rssi_data[i, j] = 100 - 20 * np.log10(distance + 1e-6) + np.random.normal(0, noise_level * 10)
            angle = np.degrees(np.arctan2(src_y - rx_y, src_x - rx_x))
            aoa_data[i, j] = angle + np.random.normal(0, noise_level * 5)

    # G·ªôp d·ªØ li·ªáu ƒë·∫ßu v√†o
    X = np.hstack([receiver_coords, rssi_data, aoa_data])
    y = source_coords

    # T·∫°o t√™n c·ªôt cho dataframe
    feature_names = []
    for j in range(num_receivers):
        feature_names.append(f'rx_{j+1}_x')
        feature_names.append(f'rx_{j+1}_y')
    for j in range(num_receivers):
        feature_names.append(f'rssi_{j+1}')
    for j in range(num_receivers):
        feature_names.append(f'aoa_{j+1}')

    df_X = pd.DataFrame(X, columns=feature_names)
    df_y = pd.DataFrame(y, columns=['source_x', 'source_y'])

    st.success("T·∫°o d·ªØ li·ªáu t·ªïng h·ª£p th√†nh c√¥ng!")
    return df_X, df_y

# --- H√†m t√≠nh kho·∫£ng c√°ch Haversine (cho t·ªça ƒë·ªô ƒë·ªãa l√Ω, gi·∫£ ƒë·ªãnh cho demo) ---
def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371000
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    return R * c

# --- Giao di·ªán Streamlit ---
st.set_page_config(layout="wide", page_title="·ª®ng d·ª•ng Hu·∫•n luy·ªán M√¥ h√¨nh ƒê·ªãnh v·ªã RF")

st.title("·ª®ng d·ª•ng Hu·∫•n luy·ªán M√¥ h√¨nh ƒê·ªãnh v·ªã Ngu·ªìn Ph√°t x·∫° RF")
st.markdown("""
·ª®ng d·ª•ng n√†y cho ph√©p b·∫°n kh√°m ph√° c√°c m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ d·ª± ƒëo√°n t·ªça ƒë·ªô ngu·ªìn ph√°t x·∫° RF.
B·∫°n c√≥ th·ªÉ t·∫°o d·ªØ li·ªáu t·ªïng h·ª£p, √°p d·ª•ng ti·ªÅn x·ª≠ l√Ω v√† hu·∫•n luy·ªán c√°c m√¥ h√¨nh kh√°c nhau.
""")

# --- Sidebar ---
st.sidebar.header("C·∫•u h√¨nh D·ªØ li·ªáu & M√¥ h√¨nh")
st.sidebar.subheader("1. T·∫°o D·ªØ li·ªáu T·ªïng h·ª£p")
num_samples = st.sidebar.slider("S·ªë l∆∞·ª£ng m·∫´u d·ªØ li·ªáu", 100, 5000, 1000)
num_receivers = st.sidebar.slider("S·ªë l∆∞·ª£ng tr·∫°m thu", 2, 10, 4)
noise_level = st.sidebar.slider("M·ª©c ƒë·ªô nhi·ªÖu", 0.0, 1.0, 0.1)

if st.sidebar.button("T·∫°o D·ªØ li·ªáu"):
    X, y = generate_synthetic_data(num_samples, num_receivers, noise_level)
    st.session_state['X'] = X
    st.session_state['y'] = y
    st.session_state['data_generated'] = True
    st.sidebar.success("D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫°o!")
else:
    if 'data_generated' not in st.session_state:
        st.session_state['data_generated'] = False

# --- Hi·ªÉn th·ªã d·ªØ li·ªáu ---
if st.session_state['data_generated']:
    st.subheader("üìä D·ªØ li·ªáu T·ªïng h·ª£p (5 d√≤ng ƒë·∫ßu ti√™n)")
    st.dataframe(st.session_state['X'].head())
    st.dataframe(st.session_state['y'].head())

    # --- Ti·ªÅn x·ª≠ l√Ω ---
    st.sidebar.subheader("2. Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu")
    use_scaler = st.sidebar.checkbox("Chu·∫©n h√≥a (StandardScaler)", True)
    use_pca = st.sidebar.checkbox("Gi·∫£m chi·ªÅu (PCA)", False)

    X_processed = st.session_state['X'].copy()
    y_processed = st.session_state['y'].copy()

    if use_scaler:
        scaler = StandardScaler()
        X_processed = pd.DataFrame(scaler.fit_transform(X_processed), columns=X_processed.columns)

    if use_pca:
        max_pca_components = min(X_processed.shape[1], num_samples - 1)
        if max_pca_components > 0:
            pca_components = st.sidebar.slider("S·ªë th√†nh ph·∫ßn PCA", 1, max_pca_components, min(5, max_pca_components))
            pca = PCA(n_components=pca_components)
            X_processed = pd.DataFrame(pca.fit_transform(X_processed))
        else:
            st.sidebar.warning("Kh√¥ng th·ªÉ √°p d·ª•ng PCA.")

    test_size = st.sidebar.slider("T·ª∑ l·ªá ki·ªÉm tra", 0.1, 0.5, 0.2, 0.05)
    X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=test_size, random_state=42)

    # --- M√¥ h√¨nh ---
    st.sidebar.subheader("3. M√¥ h√¨nh & Si√™u tham s·ªë")
    model_choice = st.sidebar.selectbox("Ch·ªçn m√¥ h√¨nh", [
        "Random Forest Regressor",
        "MLP Regressor (Neural Network)",
        "Support Vector Regressor"
    ])

    model = None
    if model_choice == "Random Forest Regressor":
        n_estimators = st.sidebar.slider("S·ªë c√¢y", 50, 500, 100, 50)
        max_depth = st.sidebar.slider("ƒê·ªô s√¢u t·ªëi ƒëa", 5, 50, 10, 5)
        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)

    elif model_choice == "MLP Regressor (Neural Network)":
        hidden_layer_sizes = st.sidebar.text_input("K√≠ch th∆∞·ªõc l·ªõp ·∫©n", "100,50")
        hidden_layer_sizes = tuple(map(int, hidden_layer_sizes.split(',')))
        max_iter = st.sidebar.slider("S·ªë v√≤ng l·∫∑p", 100, 1000, 200, 50)
        learning_rate_init = st.sidebar.slider("T·ªëc ƒë·ªô h·ªçc", 0.0001, 0.1, 0.001, 0.0001, format="%f")
        model = MLPRegressor(
            hidden_layer_sizes=hidden_layer_sizes,
            max_iter=max_iter,
            learning_rate_init=learning_rate_init,
            early_stopping=True,
            random_state=42
        )

    elif model_choice == "Support Vector Regressor":
        C = st.sidebar.slider("C", 0.1, 10.0, 1.0, 0.1)
        epsilon = st.sidebar.slider("Epsilon", 0.01, 1.0, 0.1, 0.01)
        kernel = st.sidebar.selectbox("Kernel", ["rbf", "linear", "poly"])
        model = SVR(C=C, epsilon=epsilon, kernel=kernel)

    # --- Hu·∫•n luy·ªán ---
    st.sidebar.subheader("4. Hu·∫•n luy·ªán M√¥ h√¨nh")
    if st.sidebar.button("B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán"):
        st.subheader("K·∫øt qu·∫£ Hu·∫•n luy·ªán")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)

        st.write(f"**Mean Squared Error (MSE):** {mse:.4f}")
        st.write(f"**Mean Absolute Error (MAE):** {mae:.4f}")

        # V·∫Ω bi·ªÉu ƒë·ªì scatter gi·ªØa y_test v√† y_pred
        fig, ax = plt.subplots()
        ax.scatter(y_test['source_x'], y_test['source_y'], label="Th·ª±c t·∫ø", c='blue')
        ax.scatter(y_pred[:, 0], y_pred[:, 1], label="D·ª± ƒëo√°n", c='red', alpha=0.6)
        ax.set_xlabel("X")
        ax.set_ylabel("Y")
        ax.set_title("V·ªã tr√≠ ngu·ªìn ph√°t x·∫°: Th·ª±c t·∫ø vs D·ª± ƒëo√°n")
        ax.legend()
        st.pyplot(fig)
